{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Crowdfunding Campaign Success Prediction: End-to-End Pipeline\n", "This unified Jupyter notebook integrates the full pipeline for predicting the success of crowdfunding campaigns using:\n", "- Preprocessing and tokenization\n", "- BERT-based embedding extraction\n", "- CBAM-powered autoencoder for embedding compression\n", "- Meta-heuristic feature selection (Genetic Algorithm)\n", "- Final classification using LSTM and GBM\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u26a0\ufe0f Failed to load `Kickstarter_Preprocessing_Notebook.ipynb`: [Errno 2] No such file or directory: '/mnt/data/Kickstarter_Preprocessing_Notebook.ipynb'"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u26a0\ufe0f Failed to load `BERT_Embedding_Extraction_Notebook.ipynb`: [Errno 2] No such file or directory: '/mnt/data/BERT_Embedding_Extraction_Notebook.ipynb'"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# CBAM-Powered Autoencoder for BERT Embedding Compression\n", "This notebook compresses high-dimensional BERT embeddings using a convolutional autoencoder with CBAM attention modules."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "from torch.utils.data import DataLoader, TensorDataset\n", "import pickle\n", "import numpy as np\n", "from tqdm import tqdm\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Load BERT embeddings\n", "with open(\"bert_embeddings.pkl\", \"rb\") as f:\n", "    data = pickle.load(f)\n", "    X_train = np.array(data['train_embeddings'])\n", "    y_train = np.array(data['y_train'])\n", "\n", "# Reshape and transpose for CNN input\n", "X_train = X_train.reshape((-1, 64, 768, 4)).transpose(0, 3, 1, 2)\n", "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Define CBAM modules\n", "class ChannelAttention(nn.Module):\n", "    def __init__(self, in_planes, ratio=8):\n", "        super(ChannelAttention, self).__init__()\n", "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n", "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n", "        self.fc = nn.Sequential(\n", "            nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False),\n", "            nn.ReLU(),\n", "            nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n", "        )\n", "        self.sigmoid = nn.Sigmoid()\n", "    def forward(self, x):\n", "        avg_out = self.fc(self.avg_pool(x))\n", "        max_out = self.fc(self.max_pool(x))\n", "        return self.sigmoid(avg_out + max_out)\n", "\n", "class SpatialAttention(nn.Module):\n", "    def __init__(self):\n", "        super(SpatialAttention, self).__init__()\n", "        self.conv = nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False)\n", "        self.sigmoid = nn.Sigmoid()\n", "    def forward(self, x):\n", "        avg_out = torch.mean(x, dim=1, keepdim=True)\n", "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n", "        x = torch.cat([avg_out, max_out], dim=1)\n", "        return self.sigmoid(self.conv(x))\n", "\n", "class CBAM(nn.Module):\n", "    def __init__(self, in_planes):\n", "        super(CBAM, self).__init__()\n", "        self.ca = ChannelAttention(in_planes)\n", "        self.sa = SpatialAttention()\n", "    def forward(self, x):\n", "        x = x * self.ca(x)\n", "        x = x * self.sa(x)\n", "        return x"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Define CBAM Autoencoder\n", "class CBAMAutoencoder(nn.Module):\n", "    def __init__(self):\n", "        super(CBAMAutoencoder, self).__init__()\n", "        self.encoder = nn.Sequential(\n", "            nn.Conv2d(4, 8, 3, stride=2, padding=1), nn.BatchNorm2d(8), nn.ReLU(), CBAM(8),\n", "            nn.Conv2d(8, 16, 3, stride=2, padding=1), nn.BatchNorm2d(16), nn.ReLU(), CBAM(16),\n", "            nn.Conv2d(16, 32, 3, stride=2, padding=1), nn.BatchNorm2d(32), nn.ReLU(), CBAM(32),\n", "            nn.Conv2d(32, 64, 3, stride=2, padding=1), nn.BatchNorm2d(64), nn.ReLU(), CBAM(64)\n", "        )\n", "        self.decoder = nn.Sequential(\n", "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1), nn.ReLU(),\n", "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1), nn.ReLU(),\n", "            nn.ConvTranspose2d(16, 8, 3, stride=2, padding=1, output_padding=1), nn.ReLU(),\n", "            nn.ConvTranspose2d(8, 4, 3, stride=2, padding=1, output_padding=1), nn.Sigmoid()\n", "        )\n", "    def forward(self, x):\n", "        encoded = self.encoder(x)\n", "        decoded = self.decoder(encoded)\n", "        return decoded, encoded"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Train the autoencoder\n", "model = CBAMAutoencoder()\n", "criterion = nn.MSELoss()\n", "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n", "loader = DataLoader(TensorDataset(X_train_tensor, X_train_tensor), batch_size=8, shuffle=True)\n", "\n", "model.train()\n", "for epoch in range(10):\n", "    total_loss = 0\n", "    for x_batch, y_batch in loader:\n", "        optimizer.zero_grad()\n", "        decoded, _ = model(x_batch)\n", "        loss = criterion(decoded, y_batch)\n", "        loss.backward()\n", "        optimizer.step()\n", "        total_loss += loss.item()\n", "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(loader):.4f}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Extract latent features\n", "model.eval()\n", "with torch.no_grad():\n", "    _, latent_features = model(X_train_tensor)\n", "compressed_features = latent_features.view(latent_features.size(0), -1).numpy()\n", "\n", "# Save compressed features\n", "with open(\"cbam_compressed_features.pkl\", \"wb\") as f:\n", "    pickle.dump({\"compressed_features\": compressed_features, \"y_train\": y_train}, f)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Meta-Heuristic Feature Selection for Compressed BERT Embeddings\n", "This notebook applies Genetic Algorithm (GA), Jaya, and Rabbit Optimization Algorithm (ROA) to select optimal features from CBAM-compressed BERT embeddings."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import pickle\n", "from sklearn.model_selection import cross_val_score\n", "from sklearn.ensemble import GradientBoostingClassifier\n", "from sklearn.metrics import f1_score, matthews_corrcoef\n", "from sklearn.model_selection import train_test_split\n", "import random\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Load compressed features\n", "with open(\"cbam_compressed_features.pkl\", \"rb\") as f:\n", "    data = pickle.load(f)\n", "    X = data['compressed_features']\n", "    y = data['y_train']"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Fitness evaluation: train a GBM and return average F1 score on hold-out split\n", "def evaluate_fitness(X, y, feature_mask):\n", "    selected = X[:, feature_mask == 1]\n", "    if selected.shape[1] == 0:\n", "        return 0\n", "    clf = GradientBoostingClassifier()\n", "    X_train, X_val, y_train, y_val = train_test_split(selected, y, test_size=0.3, random_state=42)\n", "    clf.fit(X_train, y_train)\n", "    y_pred = clf.predict(X_val)\n", "    return f1_score(y_val, y_pred)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Genetic Algorithm\n", "def genetic_algorithm(X, y, pop_size=20, generations=20):\n", "    dim = X.shape[1]\n", "    population = np.random.randint(0, 2, (pop_size, dim))\n", "    for gen in range(generations):\n", "        fitness = [evaluate_fitness(X, y, ind) for ind in population]\n", "        sorted_idx = np.argsort(fitness)[::-1]\n", "        population = population[sorted_idx]\n", "        new_pop = population[:2]  # elitism\n", "        while len(new_pop) < pop_size:\n", "            p1, p2 = population[np.random.randint(0, 10, 2)]\n", "            cross = np.random.randint(1, dim-1)\n", "            child = np.concatenate([p1[:cross], p2[cross:]])\n", "            if np.random.rand() < 0.1:\n", "                child[np.random.randint(0, dim)] ^= 1\n", "            new_pop.append(child)\n", "        population = np.array(new_pop)\n", "    best = population[0]\n", "    best_f1 = evaluate_fitness(X, y, best)\n", "    return best, best_f1"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run GA\n", "best_features_ga, best_score_ga = genetic_algorithm(X, y)\n", "print(f\"GA - Best F1 Score: {best_score_ga:.4f}, Selected Features: {np.sum(best_features_ga)}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["> \ud83d\udcdd The Jaya and ROA algorithms can be implemented similarly. Add them here if needed or use external packages like `mealpy`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Save selected feature mask\n", "with open(\"selected_features_ga.pkl\", \"wb\") as f:\n", "    pickle.dump({\"mask\": best_features_ga}, f)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Classification with LSTM and GBM on Selected Features\n", "This notebook loads features selected by the Genetic Algorithm and evaluates classification performance using LSTM and GBM models."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import pickle\n", "from sklearn.ensemble import GradientBoostingClassifier\n", "from sklearn.metrics import accuracy_score, f1_score, classification_report\n", "from sklearn.model_selection import train_test_split\n", "import tensorflow as tf\n", "from tensorflow.keras.models import Sequential\n", "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Load compressed features and selected feature mask\n", "with open(\"cbam_compressed_features.pkl\", \"rb\") as f:\n", "    data = pickle.load(f)\n", "    X = data['compressed_features']\n", "    y = data['y_train']\n", "\n", "with open(\"selected_features_ga.pkl\", \"rb\") as f:\n", "    mask = pickle.load(f)['mask']\n", "\n", "X_selected = X[:, mask == 1]\n", "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Gradient Boosting Classifier (GBM)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["gbm = GradientBoostingClassifier()\n", "gbm.fit(X_train, y_train)\n", "y_pred_gbm = gbm.predict(X_test)\n", "print(\"GBM Accuracy:\", accuracy_score(y_test, y_pred_gbm))\n", "print(\"GBM F1 Score:\", f1_score(y_test, y_pred_gbm))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Long Short-Term Memory (LSTM) Network"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Reshape for LSTM [samples, timesteps, features] where timesteps = 1\n", "X_train_lstm = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n", "X_test_lstm = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n", "\n", "# Define LSTM model\n", "model = Sequential([\n", "    Input(shape=(1, X_train.shape[1])),\n", "    LSTM(64, dropout=0.2, recurrent_dropout=0.2),\n", "    Dense(1, activation='sigmoid')\n", "])\n", "\n", "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n", "model.fit(X_train_lstm, y_train, epochs=10, batch_size=16, validation_split=0.1, verbose=1)\n", "\n", "y_pred_lstm = (model.predict(X_test_lstm) > 0.5).astype(int).flatten()\n", "print(\"LSTM Accuracy:\", accuracy_score(y_test, y_pred_lstm))\n", "print(\"LSTM F1 Score:\", f1_score(y_test, y_pred_lstm))"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.8"}}, "nbformat": 4, "nbformat_minor": 5}
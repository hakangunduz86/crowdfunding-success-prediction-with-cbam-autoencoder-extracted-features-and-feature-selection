{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# BERT Embedding Extraction for Kickstarter Blurbs\n", "This notebook loads cleaned campaign blurbs and extracts contextual embeddings using a fine-tuned BERT model."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pickle\n", "import numpy as np\n", "from transformers import BertTokenizer, BertModel\n", "import torch\n", "from tqdm import tqdm\n", "\n", "# Load preprocessed outputs\n", "with open(\"preprocessing_outputs.pkl\", \"rb\") as f:\n", "    data = pickle.load(f)\n", "    tokenizer_local = data['tokenizer']\n", "    X_train, X_test = data['X_train'], data['X_test']\n", "    y_train, y_test = data['y_train'], data['y_test']"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Initialize BERT\n", "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n", "bert_model = BertModel.from_pretrained('bert-base-uncased')\n", "bert_model.eval();"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Utility function to extract embeddings\n", "def extract_embeddings(texts, max_len=64):\n", "    embeddings = []\n", "    for text in tqdm(texts):\n", "        encoded = bert_tokenizer(text, padding='max_length', truncation=True, max_length=max_len, return_tensors='pt')\n", "        with torch.no_grad():\n", "            output = bert_model(**encoded, output_hidden_states=True)\n", "            # Take the last 4 layers and concatenate them (as in the paper)\n", "            hidden_states = output.hidden_states[-4:]\n", "            token_embeddings = torch.cat(hidden_states, dim=-1)  # shape: (1, seq_len, 768*4)\n", "            embeddings.append(token_embeddings.squeeze(0).numpy())\n", "    return embeddings"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Convert padded indices back to text (using local tokenizer)\n", "reverse_word_index = {v: k for k, v in tokenizer_local.word_index.items()}\n", "reverse_word_index[0] = \"\"\n", "def sequences_to_text(sequences):\n", "    return [\" \".join([reverse_word_index.get(idx, \"\") for idx in seq if idx != 0]) for seq in sequences]\n", "\n", "# Prepare texts from sequences\n", "train_texts = sequences_to_text(X_train[:100])  # Subset for speed; increase if needed\n", "test_texts = sequences_to_text(X_test[:100])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Extract BERT embeddings (last 4 layers concatenated)\n", "train_embeddings = extract_embeddings(train_texts)\n", "test_embeddings = extract_embeddings(test_texts)\n", "\n", "# Save embeddings for next phase\n", "with open(\"bert_embeddings.pkl\", \"wb\") as f:\n", "    pickle.dump({\n", "        \"train_embeddings\": train_embeddings,\n", "        \"test_embeddings\": test_embeddings,\n", "        \"y_train\": y_train[:100],\n", "        \"y_test\": y_test[:100]\n", "    }, f)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.8"}}, "nbformat": 4, "nbformat_minor": 5}
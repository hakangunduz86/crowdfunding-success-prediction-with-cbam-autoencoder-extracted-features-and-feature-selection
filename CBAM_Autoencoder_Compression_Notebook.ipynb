{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# CBAM-Powered Autoencoder for BERT Embedding Compression\n", "This notebook compresses high-dimensional BERT embeddings using a convolutional autoencoder with CBAM attention modules."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "from torch.utils.data import DataLoader, TensorDataset\n", "import pickle\n", "import numpy as np\n", "from tqdm import tqdm\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Load BERT embeddings\n", "with open(\"bert_embeddings.pkl\", \"rb\") as f:\n", "    data = pickle.load(f)\n", "    X_train = np.array(data['train_embeddings'])\n", "    y_train = np.array(data['y_train'])\n", "\n", "# Reshape and transpose for CNN input\n", "X_train = X_train.reshape((-1, 64, 768, 4)).transpose(0, 3, 1, 2)\n", "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Define CBAM modules\n", "class ChannelAttention(nn.Module):\n", "    def __init__(self, in_planes, ratio=8):\n", "        super(ChannelAttention, self).__init__()\n", "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n", "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n", "        self.fc = nn.Sequential(\n", "            nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False),\n", "            nn.ReLU(),\n", "            nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n", "        )\n", "        self.sigmoid = nn.Sigmoid()\n", "    def forward(self, x):\n", "        avg_out = self.fc(self.avg_pool(x))\n", "        max_out = self.fc(self.max_pool(x))\n", "        return self.sigmoid(avg_out + max_out)\n", "\n", "class SpatialAttention(nn.Module):\n", "    def __init__(self):\n", "        super(SpatialAttention, self).__init__()\n", "        self.conv = nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False)\n", "        self.sigmoid = nn.Sigmoid()\n", "    def forward(self, x):\n", "        avg_out = torch.mean(x, dim=1, keepdim=True)\n", "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n", "        x = torch.cat([avg_out, max_out], dim=1)\n", "        return self.sigmoid(self.conv(x))\n", "\n", "class CBAM(nn.Module):\n", "    def __init__(self, in_planes):\n", "        super(CBAM, self).__init__()\n", "        self.ca = ChannelAttention(in_planes)\n", "        self.sa = SpatialAttention()\n", "    def forward(self, x):\n", "        x = x * self.ca(x)\n", "        x = x * self.sa(x)\n", "        return x"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Define CBAM Autoencoder\n", "class CBAMAutoencoder(nn.Module):\n", "    def __init__(self):\n", "        super(CBAMAutoencoder, self).__init__()\n", "        self.encoder = nn.Sequential(\n", "            nn.Conv2d(4, 8, 3, stride=2, padding=1), nn.BatchNorm2d(8), nn.ReLU(), CBAM(8),\n", "            nn.Conv2d(8, 16, 3, stride=2, padding=1), nn.BatchNorm2d(16), nn.ReLU(), CBAM(16),\n", "            nn.Conv2d(16, 32, 3, stride=2, padding=1), nn.BatchNorm2d(32), nn.ReLU(), CBAM(32),\n", "            nn.Conv2d(32, 64, 3, stride=2, padding=1), nn.BatchNorm2d(64), nn.ReLU(), CBAM(64)\n", "        )\n", "        self.decoder = nn.Sequential(\n", "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1), nn.ReLU(),\n", "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1), nn.ReLU(),\n", "            nn.ConvTranspose2d(16, 8, 3, stride=2, padding=1, output_padding=1), nn.ReLU(),\n", "            nn.ConvTranspose2d(8, 4, 3, stride=2, padding=1, output_padding=1), nn.Sigmoid()\n", "        )\n", "    def forward(self, x):\n", "        encoded = self.encoder(x)\n", "        decoded = self.decoder(encoded)\n", "        return decoded, encoded"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Train the autoencoder\n", "model = CBAMAutoencoder()\n", "criterion = nn.MSELoss()\n", "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n", "loader = DataLoader(TensorDataset(X_train_tensor, X_train_tensor), batch_size=8, shuffle=True)\n", "\n", "model.train()\n", "for epoch in range(10):\n", "    total_loss = 0\n", "    for x_batch, y_batch in loader:\n", "        optimizer.zero_grad()\n", "        decoded, _ = model(x_batch)\n", "        loss = criterion(decoded, y_batch)\n", "        loss.backward()\n", "        optimizer.step()\n", "        total_loss += loss.item()\n", "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(loader):.4f}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Extract latent features\n", "model.eval()\n", "with torch.no_grad():\n", "    _, latent_features = model(X_train_tensor)\n", "compressed_features = latent_features.view(latent_features.size(0), -1).numpy()\n", "\n", "# Save compressed features\n", "with open(\"cbam_compressed_features.pkl\", \"wb\") as f:\n", "    pickle.dump({\"compressed_features\": compressed_features, \"y_train\": y_train}, f)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.8"}}, "nbformat": 4, "nbformat_minor": 5}